{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46251a5f",
   "metadata": {},
   "source": [
    "API connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf80ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\") or getpass(\"Enter API key:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28cecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"openai/gpt-oss-20b\", model_provider=\"groq\", temperature=0.0)\n",
    "creative_llm = init_chat_model(\"openai/gpt-oss-20b\", model_provider=\"groq\", temperature=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c8efcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
    "\n",
    "# Initialize search tool\n",
    "search = GoogleSerperAPIWrapper(serper_api_key=serper_api_key)\n",
    "\n",
    "\n",
    "@tool\n",
    "def google_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrievs information through google search\n",
    "    \"\"\"\n",
    "    result = search.run(query)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "289267a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def set_llm(api_key=None, api_key_base=None, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Initialize LLM and store it in st.session_state.llm_instance.\n",
    "    Returns True if successful.\n",
    "    \"\"\"\n",
    "    api_key\n",
    "    if not api_key:\n",
    "        raise ValueError(\"âš ï¸ API key not provided!\")\n",
    "\n",
    "    # Initialize LLM and store in session\n",
    "    return ChatOpenAI(\n",
    "        model_name=model,\n",
    "        temperature=0,\n",
    "        openai_api_key=api_key,\n",
    "        openai_api_base=api_key_base,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aab4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease_info_tool.py\n",
    "from langchain.tools import tool\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "embeddings = get_embeddings()  # or HuggingFaceEmbeddings() if thatâ€™s your fn\n",
    "df = pd.read_csv(MAYO_CSV)\n",
    "vectorstore = FAISS.load_local(\n",
    "    DISEASE_INFO_FAISS_DB, embeddings, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "\n",
    "@tool\n",
    "def match_disease_info(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve top-3 matching diseases from the FAISS DB\n",
    "    and return a short overview for each.\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "    if not results:\n",
    "        return \"No close matches found.\"\n",
    "\n",
    "    out = []\n",
    "    for r in results:\n",
    "        disease = r.metadata.get(\"disease\", \"Unknown\")\n",
    "        # get Overview from the CSV\n",
    "        overview_series = df.loc[df[\"disease\"] == disease, \"Overview\"]\n",
    "        symptoms_series = df.loc[df[\"disease\"] == disease, \"Symptoms\"]\n",
    "\n",
    "        overview = (\n",
    "            overview_series.values[0]\n",
    "            if not overview_series.empty\n",
    "            else \"Overview not found.\"\n",
    "        )\n",
    "        symptoms = (\n",
    "            symptoms_series.values[0]\n",
    "            if not symptoms_series.empty\n",
    "            else \"Symptom not found.\"\n",
    "        )\n",
    "\n",
    "        out.append(f\"Disease: {disease}\\nOverview: {overview}\\nSymptoms: {symptoms}\")\n",
    "\n",
    "    return \"\\n\\n\".join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
    "\n",
    "# Initialize search tool\n",
    "search = GoogleSerperAPIWrapper(serper_api_key=serper_api_key)\n",
    "\n",
    "\n",
    "@tool\n",
    "def google_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrievs information through google search\n",
    "    \"\"\"\n",
    "    result = search.run(query)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec09eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "\n",
    "\n",
    "class DISEASEINFOAGENT:\n",
    "    def __init__(self, llm):\n",
    "        self.system_prompt = \"\"\"\n",
    "        You're a helpful assistant. When you get a query, you should be able to extract\n",
    "        disease info using any of the tools given.\n",
    "        \"\"\"\n",
    "        self.tools = [match_disease_info, google_search]\n",
    "        self.llm = llm\n",
    "\n",
    "        # create the prompt template\n",
    "        self.prompt_template = ChatPromptTemplate.from_messages(\n",
    "            [(\"system\", self.system_prompt), (\"human\", \"{query}\")]\n",
    "        )\n",
    "\n",
    "        # create the agent once\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\"query\": lambda x: x[\"query\"]}\n",
    "            | self.prompt_template\n",
    "            | self.llm.bind_tools(self.tools, tool_choice=\"any\")\n",
    "        )\n",
    "\n",
    "    def invoke(self, query):\n",
    "        response_str = self.agent.invoke({\"query\": query})\n",
    "        return response_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fb3cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='disease_info' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 153, 'prompt_tokens': 174, 'total_tokens': 327, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.052476069, 'prompt_time': 0.008725021, 'completion_time': 0.303980181, 'total_time': 0.312705202}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_3a688838c3', 'id': 'chatcmpl-f8731278-7f46-4732-b37b-984b84e3282c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--3d27f1bf-006a-4997-95f4-9d7728cddd8d-0' usage_metadata={'input_tokens': 174, 'output_tokens': 153, 'total_tokens': 327, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = set_llm(os.getenv(\"GROQ_API_KEY\"), os.getenv(\"TEST_API_BASE\"), os.getenv(\"TEST_MODEL\") )\n",
    "d =DECIDERAGENT(llm)\n",
    "\n",
    "print(d.invoke(\"what is common cold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08cec009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! ðŸŒ How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"Hello WOrld\". It\\'s likely a greeting. Should respond politely. Possibly mention the assistant name? The user says \"Hello WOrld\". The correct spelling would be \"Hello World\". Could respond with greeting and ask how can help. Should I correct the spelling? The user typed \"WOrld\" with capital O in middle. It\\'s okay to correct politely. Let\\'s greet.'}, response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 75, 'total_tokens': 177, 'completion_time': 0.09254194, 'prompt_time': 0.004819386, 'queue_time': 0.046388904, 'total_time': 0.097361326}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_a5ac2a5d7b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a241ed8f-7a63-40f6-9c61-f9560d37686a-0', usage_metadata={'input_tokens': 75, 'output_tokens': 102, 'total_tokens': 177})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creative_llm.invoke(\"Hello WOrld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65485db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "If there is no enemy within, the enemy outside can do us no harm.â€1\n",
    "\n",
    "Ever since a Hezbollah suicide bomber in 1983 blew up a truck packed with explosives and killed 241 Marines in Beirut, combating Islamic terrorist organizations has been a priority for U.S. intelligence, security, and law enforcement agencies. However, for those of us who have followed the spiraling growth of Islamic terrorism in the 1980s and 1990s, it seemed as if the U.S was sluggishly reactive. They made little headway in extensive counterterrorism programs designed at penetrating and dismantling Islamic terror groups.2\n",
    "\n",
    "How was it possible for the hijackers and their plot to remain off the radar of intelligence and law enforcement?\n",
    "The 9/11 attack put a spotlight on the failures of the security agencies tasked to protect the U.S. against acts of terror. How was it possible for 19 hijackers and their ambitious plot to remain off the radar of intelligence and law enforcement? The truth, as I discovered during 18 months of reporting for my book, Why America Slept: The Failure to Prevent 9/11, was not that the plot had gone undetected, but rather that the agencies responsible for monitoring and fighting terrorism had failed to share information, something that would have made it possible to connect the dots before the attack occurred.\n",
    "\n",
    "The failures were more substantive than mere interagency rivalries between the CIA, FBI, NSA, and local law enforcement. Exclusive interviews I had with top intelligence officers and FBI officials revealed that the origins and depth of the dysfunction inside Americaâ€™s counterintelligence programs was an internecine bureaucratic war that left little room for working together. Sharing information was given lip service but seldom practiced, particularly when the intelligence at stake was judged as having â€œhigh value.â€\n",
    "\n",
    "If the CIA had alerted the State Department, the two Saudis would have been on a watch list that barred them from entering the United States.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "857b0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant and your name is {name} that helps generate article titles\",\n",
    "    input_variables=[\"name\"]\n",
    ")\n",
    "\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a name for a article. The article is here for you to examine:\n",
    "\n",
    "    ---\n",
    "\n",
    "    {article}\n",
    "\n",
    "    ---\n",
    "\n",
    "    The name should be based of the conext of the article. Be creative, but make sure the names are clear, catchy,\n",
    "    and relavant to the theme of the article.\n",
    "\n",
    "    Only output the article name, no other explaination or\n",
    "    text   can be provided. \"\"\",\n",
    "    input_variables=[\"article\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa04b81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are tasked with creating a name for a article. The article is here for you to examine:\\n\\n    ---\\n\\n    TEST STRING\\n\\n    ---\\n\\n    The name should be based of the conext of the article. Be creative, but make sure the names are clear, catchy,\\n    and relavant to the theme of the article.\\n\\n    Only output the article name, no other explaination or\\n    text   can be provided. '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt.format(name=\"joe\",article=\"TEST STRING\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9df6ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec83aef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are an AI assistant and your name is joe that helps generate article titles\\nHuman: You are tasked with creating a name for a article. The article is here for you to examine:\\n\\n    ---\\n\\n    TEST_STRING\\n\\n    ---\\n\\n    The name should be based of the conext of the article. Be creative, but make sure the names are clear, catchy,\\n    and relavant to the theme of the article.\\n\\n    Only output the article name, no other explaination or\\n    text   can be provided. '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_prompt.format(name=\"joe\",article=\"TEST_STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_one = (\n",
    "    {\"article\": lambda x: x[\"article\"],\n",
    "     \"name\": lambda x:x[\"name\"]}\n",
    "    | first_prompt\n",
    "    | creative_llm\n",
    "    | {\"article_title\": lambda x: x.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0eddfc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title_msg= chain_one.invoke({\n",
    "    \"article\": article,\n",
    "    \"name\": \"Joe\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a95e7d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_title': 'The Silent War Within: How U.S. Intelligence Silos Allowed 9/11'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_title_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08bd945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that helps build good articles\"\n",
    ")\n",
    "\n",
    "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a descripion for the\n",
    "      article. The article is here for you to examine:\n",
    "\n",
    "    ---\n",
    "\n",
    "    {article}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Here is the article title: {article_title}\n",
    "\n",
    "    Ouput SEO friendly article description. Do not output anything other than description.\n",
    "\n",
    "\"\"\",\n",
    "    input_variables=[\"article\", \"article_title\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98286fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt= ChatPromptTemplate([system_prompt, second_user_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc5c5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_two = (\n",
    "    {\"article\": lambda x:x[\"article\"],\n",
    "     \"article_title\": lambda x:x[\"article_title\"]}\n",
    "    | second_prompt\n",
    "    | llm\n",
    "    | {\"Summary\": lambda x:x.content}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a440b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_msg = chain_two.invoke(\n",
    "    {\"article\" : article,\n",
    "     \"article_title\" :      article_title_msg[\"article_title\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04aebb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Summary': 'Explore how U.S. intelligence silos and interagency rivalry let 9/11 terrorists slip through the cracks, revealing a failure of information sharing and counterterrorism.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c9e27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Hi, I am Harmesh. I am currently pursuing my B.Tech in Computer Science at Lovely Professional University. I specialize in Data Science and Machine Learning.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that helps to review and rewrite given paragraph\"\n",
    ")\n",
    "\n",
    "third_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with reviewing the paragraph\n",
    "       and rewrite it with proper grammar and wrting. The paragraph is here for you to examine:\n",
    "\n",
    "    ---\n",
    "\n",
    "    {paragraph}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Choose one paragraph to review and edit. During your edit ensure you provide\n",
    "    constructive feedback to the user so they can learn where to improve their own writing.,\n",
    "\n",
    "\n",
    "\"\"\",\n",
    "    input_variables=[\"paragraph\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3c86f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_prompt = ChatPromptTemplate([third_system_prompt, third_user_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e80f85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/Documents/RAG-Experiments/.venv/lib/python3.12/site-packages/pydantic/json_schema.py:2324: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, json_schema_extra={'descripion': 'The original Paragraph'}),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "/home/ml/Documents/RAG-Experiments/.venv/lib/python3.12/site-packages/pydantic/json_schema.py:2324: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='The improved edited Paragraph'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Paragraph(BaseModel):\n",
    "    original_paragraph: str = Field(descripion = \"The original Paragraph\"),\n",
    "    edited_paragraph: str = Field(description=\"The improved edited Paragraph\"),\n",
    "    feedback: int = Field(description=\"Constructive feedback about the original paragrpah out of 100\")\n",
    "\n",
    "structured_llm = creative_llm.with_structured_output(Paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8a6a1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_three= (\n",
    "    {\"paragraph\": lambda x:x[\"paragraph\"]}\n",
    "    | third_prompt\n",
    "    | structured_llm\n",
    "    | {\"output\": lambda x:x.edited_paragraph}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "23fd7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_msg = chain_three.invoke({\"paragraph\":paragraph})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc26c677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'Hello, Iâ€™m Harmesh. I am currently pursuing a B.Tech in Computer Science at Lovely Professional University, where I specialize in Data Science and Machine Learning.'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef09b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "image_prompt = PromptTemplate(\n",
    "    input_variables=[\"article\"],\n",
    "    template=\"\"\"\n",
    "    Generate a prompt with less than 500 characters to generate an image\n",
    "    based on the following article ; {article}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1e3cd",
   "metadata": {},
   "source": [
    "## LangChain's Memory Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff8c33",
   "metadata": {},
   "source": [
    "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
    "\n",
    "Throughout the notebook we will be referring to these _older_ memory types and then rewriting them using the recommended `RunnableWithMessageHistory` class. We will learn about:\n",
    "\n",
    "* `ConversationBufferMemory`: the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
    "* `ConversationBufferWindowMemory`: similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages.\n",
    "* `ConversationSummaryMemory`: rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
    "* `ConversationSummaryBufferMemory`: merges the `ConversationSummaryMemory` and `ConversationTokenBufferMemory` types.\n",
    "\n",
    "We'll work through each of these memory types in turn, and rewrite each one using the `RunnableWithMessageHistory` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc9e58",
   "metadata": {},
   "source": [
    "## 1. `ConversationBufferMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28088945",
   "metadata": {},
   "source": [
    "`ConversationBufferMemory` is the simplest form of conversational memory, it is _literally_ just a place that we store messages, and then use to feed messages into our LLM.\n",
    "\n",
    "Let's start with LangChain's original `ConversationBufferMemory` object, we are setting `return_messages=True` to return the messages as a list of `ChatMessage` objects â€” unless using a non-chat model we would always set this to `True` as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895432a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc023801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108827/1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1404f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my name is James\"},  # user message\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
    "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
    "    {\"output\": \"Very cool!\"}  # AI response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9939394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392e770e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d1119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108827/839683240.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b68dce48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Youâ€™re James!  (You mentioned that at the very start of our chat.)', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Youâ€™re James!  (You mentioned that at the very start of our chat.)'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad38a19",
   "metadata": {},
   "source": [
    "### `ConversationBufferMemory` with `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4a2833",
   "metadata": {},
   "source": [
    "As mentioned, the `ConversationBufferMemory` type is due for deprecation. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
    "\n",
    "When implementing `RunnableWithMessageHistory` we will use **L**ang**C**hain **E**xpression **L**anguage (LCEL) and for this we need to define our prompt template and LLM components. Our `llm` has already been defined, so now we just define a `ChatPromptTemplate` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62b2c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f7f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51dafaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfc320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
